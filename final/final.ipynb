{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"a_affirmative.csv\")\n",
    "data = data.drop(data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.25)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(model):\n",
    "    print(\"Statistics on Testing\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"True Negative:\", tn, \"False Positives:\", fp, \"False Negatives:\", fn, \"True Positives:\", tp)\n",
    "    accuracy = model.score(X_test, y_true)\n",
    "    error = 1 - accuracy\n",
    "    print(\"Accuracy:\", accuracy, \"Error:\", error)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)\n",
    "    print(\"Statistics on Training\")\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"True Negative:\", tn, \"False Positives:\", fp, \"False Negatives:\", fn, \"True Positives:\", tp)\n",
    "    accuracy = model.score(X, y)\n",
    "    error = 1 - accuracy\n",
    "    print(\"Accuracy:\", accuracy, \"Error:\", error)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 165 False Positives: 4 False Negatives: 17 True Positives: 80\n",
      "Accuracy: 0.9210526315789473 Error: 0.07894736842105265\n",
      "Precision: 0.9523809523809523 Recall: 0.8247422680412371 F1: 0.883977900552486\n",
      "Statistics on Training\n",
      "True Negative: 478 False Positives: 1 False Negatives: 3 True Positives: 314\n",
      "Accuracy: 0.9949748743718593 Error: 0.005025125628140725\n",
      "Precision: 0.9968253968253968 Recall: 0.9905362776025236 F1: 0.9936708860759493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X,y)\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 163 False Positives: 6 False Negatives: 13 True Positives: 84\n",
      "Accuracy: 0.9285714285714286 Error: 0.0714285714285714\n",
      "Precision: 0.9333333333333333 Recall: 0.865979381443299 F1: 0.8983957219251337\n",
      "Statistics on Training\n",
      "True Negative: 479 False Positives: 0 False Negatives: 0 True Positives: 317\n",
      "Accuracy: 1.0 Error: 0.0\n",
      "Precision: 1.0 Recall: 1.0 F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=50)\n",
    "model2.fit(X,y)\n",
    "print_stats(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 56y (0.079645)\n",
      "2. feature 51y (0.044687)\n",
      "3. feature 64y (0.044446)\n",
      "4. feature 65y (0.043404)\n",
      "5. feature 46y (0.042312)\n",
      "6. feature 37y (0.042229)\n",
      "7. feature 5y (0.039268)\n",
      "8. feature 44y (0.038877)\n",
      "9. feature 45y (0.038641)\n",
      "10. feature 48z (0.015025)\n",
      "11. feature 43y (0.013527)\n",
      "12. feature 73x (0.011465)\n",
      "13. feature 60z (0.010808)\n",
      "14. feature 98y (0.010480)\n",
      "15. feature 35x (0.009939)\n",
      "16. feature 8y (0.009609)\n",
      "17. feature 76x (0.009079)\n",
      "18. feature 30y (0.008910)\n",
      "19. feature 0y (0.008857)\n",
      "20. feature 22y (0.008350)\n",
      "21. feature 50y (0.007831)\n",
      "22. feature 3z (0.007378)\n",
      "23. feature 42y (0.007099)\n",
      "24. feature 75y (0.007084)\n",
      "25. feature 59y (0.007067)\n",
      "26. feature 82y (0.006976)\n",
      "27. feature 68x (0.006865)\n",
      "28. feature 76y (0.006756)\n",
      "29. feature 57y (0.006749)\n",
      "30. feature 74z (0.006724)\n",
      "31. feature 55y (0.006434)\n",
      "32. feature 72x (0.006159)\n",
      "33. feature 19z (0.005791)\n",
      "34. feature 79z (0.005738)\n",
      "35. feature 91y (0.005467)\n",
      "36. feature 81y (0.005357)\n",
      "37. feature 34x (0.005133)\n",
      "38. feature 60x (0.005077)\n",
      "39. feature 54x (0.005066)\n",
      "40. feature 78y (0.005049)\n",
      "41. feature 92y (0.004982)\n",
      "42. feature 70x (0.004961)\n",
      "43. feature 4x (0.004857)\n",
      "44. feature 16y (0.004837)\n",
      "45. feature 25y (0.004768)\n",
      "46. feature 15y (0.004731)\n",
      "47. feature 44x (0.004567)\n",
      "48. feature 39y (0.004520)\n",
      "49. feature 99y (0.004470)\n",
      "50. feature 13y (0.004430)\n",
      "51. feature 41y (0.004326)\n",
      "52. feature 40y (0.004156)\n",
      "53. feature 19y (0.004007)\n",
      "54. feature 20y (0.003925)\n",
      "55. feature 99x (0.003908)\n",
      "56. feature 60y (0.003855)\n",
      "57. feature 48x (0.003703)\n",
      "58. feature 51x (0.003686)\n",
      "59. feature 35y (0.003627)\n",
      "60. feature 31y (0.003602)\n",
      "61. feature 29y (0.003598)\n",
      "62. feature 81z (0.003562)\n",
      "63. feature 63y (0.003510)\n",
      "64. feature 75x (0.003503)\n",
      "65. feature 97y (0.003487)\n",
      "66. feature 43z (0.003478)\n",
      "67. feature 25z (0.003450)\n",
      "68. feature 49z (0.003418)\n",
      "69. feature 87y (0.003316)\n",
      "70. feature 95y (0.003313)\n",
      "71. feature 33y (0.003275)\n",
      "72. feature 6y (0.003257)\n",
      "73. feature 84z (0.003195)\n",
      "74. feature 88y (0.003118)\n",
      "75. feature 98x (0.003032)\n",
      "76. feature 91z (0.003004)\n",
      "77. feature 89y (0.002973)\n",
      "78. feature 73y (0.002964)\n",
      "79. feature 90y (0.002900)\n",
      "80. feature 62y (0.002886)\n",
      "81. feature 64z (0.002858)\n",
      "82. feature 70y (0.002848)\n",
      "83. feature 79x (0.002805)\n",
      "84. feature 4y (0.002738)\n",
      "85. feature 48y (0.002699)\n",
      "86. feature 8z (0.002642)\n",
      "87. feature 15x (0.002546)\n",
      "88. feature 74x (0.002507)\n",
      "89. feature 36x (0.002506)\n",
      "90. feature 85y (0.002493)\n",
      "91. feature 10y (0.002489)\n",
      "92. feature 54z (0.002483)\n",
      "93. feature 90z (0.002468)\n",
      "94. feature 14y (0.002442)\n",
      "95. feature 66z (0.002437)\n",
      "96. feature 32y (0.002414)\n",
      "97. feature 67x (0.002413)\n",
      "98. feature 2x (0.002388)\n",
      "99. feature 1x (0.002378)\n",
      "100. feature 58x (0.002369)\n",
      "101. feature 52y (0.002318)\n",
      "102. feature 69z (0.002260)\n",
      "103. feature 16z (0.002238)\n",
      "104. feature 78x (0.002220)\n",
      "105. feature 41z (0.002151)\n",
      "106. feature 3x (0.002141)\n",
      "107. feature 39x (0.002129)\n",
      "108. feature 52x (0.002126)\n",
      "109. feature 84x (0.002114)\n",
      "110. feature 23y (0.002104)\n",
      "111. feature 32x (0.002077)\n",
      "112. feature 77y (0.002069)\n",
      "113. feature 69y (0.002041)\n",
      "114. feature 97z (0.002024)\n",
      "115. feature 52z (0.002024)\n",
      "116. feature 36y (0.001952)\n",
      "117. feature 98z (0.001946)\n",
      "118. feature 26y (0.001937)\n",
      "119. feature 47x (0.001933)\n",
      "120. feature 96x (0.001932)\n",
      "121. feature 67y (0.001927)\n",
      "122. feature 26x (0.001881)\n",
      "123. feature 15z (0.001842)\n",
      "124. feature 38x (0.001810)\n",
      "125. feature 27y (0.001763)\n",
      "126. feature 7y (0.001709)\n",
      "127. feature 92x (0.001691)\n",
      "128. feature 94x (0.001689)\n",
      "129. feature 90x (0.001679)\n",
      "130. feature 1z (0.001664)\n",
      "131. feature 80x (0.001662)\n",
      "132. feature 17z (0.001610)\n",
      "133. feature 93y (0.001608)\n",
      "134. feature 85x (0.001596)\n",
      "135. feature 96y (0.001577)\n",
      "136. feature 50x (0.001543)\n",
      "137. feature 86y (0.001531)\n",
      "138. feature 12x (0.001488)\n",
      "139. feature 9x (0.001450)\n",
      "140. feature 73z (0.001449)\n",
      "141. feature 28y (0.001405)\n",
      "142. feature 24y (0.001401)\n",
      "143. feature 20x (0.001398)\n",
      "144. feature 71x (0.001398)\n",
      "145. feature 11y (0.001379)\n",
      "146. feature 7x (0.001376)\n",
      "147. feature 12y (0.001359)\n",
      "148. feature 86x (0.001358)\n",
      "149. feature 49y (0.001356)\n",
      "150. feature 37z (0.001318)\n",
      "151. feature 28x (0.001271)\n",
      "152. feature 72y (0.001254)\n",
      "153. feature 92z (0.001254)\n",
      "154. feature 80y (0.001243)\n",
      "155. feature 19x (0.001236)\n",
      "156. feature 18z (0.001218)\n",
      "157. feature 68y (0.001212)\n",
      "158. feature 47y (0.001187)\n",
      "159. feature 84y (0.001175)\n",
      "160. feature 89x (0.001129)\n",
      "161. feature 81x (0.001120)\n",
      "162. feature 71y (0.001110)\n",
      "163. feature 62z (0.001090)\n",
      "164. feature 56z (0.001078)\n",
      "165. feature 34y (0.001058)\n",
      "166. feature 39z (0.001044)\n",
      "167. feature 14x (0.000978)\n",
      "168. feature 55x (0.000959)\n",
      "169. feature 32z (0.000937)\n",
      "170. feature 30x (0.000930)\n",
      "171. feature 38y (0.000923)\n",
      "172. feature 54y (0.000921)\n",
      "173. feature 65x (0.000912)\n",
      "174. feature 8x (0.000906)\n",
      "175. feature 22x (0.000880)\n",
      "176. feature 53y (0.000863)\n",
      "177. feature 67z (0.000831)\n",
      "178. feature 74y (0.000829)\n",
      "179. feature 55z (0.000800)\n",
      "180. feature 64x (0.000787)\n",
      "181. feature 77z (0.000755)\n",
      "182. feature 78z (0.000751)\n",
      "183. feature 21y (0.000725)\n",
      "184. feature 94y (0.000704)\n",
      "185. feature 41x (0.000703)\n",
      "186. feature 27x (0.000701)\n",
      "187. feature 99z (0.000692)\n",
      "188. feature 75z (0.000691)\n",
      "189. feature 0x (0.000670)\n",
      "190. feature 24x (0.000660)\n",
      "191. feature 33x (0.000643)\n",
      "192. feature 30z (0.000638)\n",
      "193. feature 97x (0.000593)\n",
      "194. feature 83x (0.000522)\n",
      "195. feature 40z (0.000522)\n",
      "196. feature 89z (0.000518)\n",
      "197. feature 3y (0.000512)\n",
      "198. feature 42z (0.000510)\n",
      "199. feature 31x (0.000499)\n",
      "200. feature 69x (0.000493)\n",
      "201. feature 25x (0.000489)\n",
      "202. feature 56x (0.000482)\n",
      "203. feature 27z (0.000482)\n",
      "204. feature 86z (0.000480)\n",
      "205. feature 10x (0.000472)\n",
      "206. feature 23x (0.000471)\n",
      "207. feature 66y (0.000447)\n",
      "208. feature 13x (0.000439)\n",
      "209. feature 58y (0.000420)\n",
      "210. feature 37x (0.000416)\n",
      "211. feature 18x (0.000405)\n",
      "212. feature 49x (0.000402)\n",
      "213. feature 2z (0.000402)\n",
      "214. feature 26z (0.000400)\n",
      "215. feature 62x (0.000394)\n",
      "216. feature 57z (0.000392)\n",
      "217. feature 53z (0.000387)\n",
      "218. feature 77x (0.000353)\n",
      "219. feature 57x (0.000351)\n",
      "220. feature 38z (0.000329)\n",
      "221. feature 21x (0.000264)\n",
      "222. feature 36z (0.000236)\n",
      "223. feature 5z (0.000212)\n",
      "224. feature 82z (0.000161)\n",
      "225. feature 43x (0.000161)\n",
      "226. feature 0z (0.000135)\n",
      "227. feature 6x (0.000122)\n",
      "228. feature 93x (0.000113)\n",
      "229. feature 35z (0.000109)\n",
      "230. feature 24z (0.000097)\n",
      "231. feature 7z (0.000089)\n",
      "232. feature 79y (0.000060)\n",
      "233. feature 59x (0.000056)\n",
      "234. feature 63z (0.000037)\n",
      "235. feature 70z (0.000019)\n",
      "236. feature 9z (0.000000)\n",
      "237. feature 87z (0.000000)\n",
      "238. feature 9y (0.000000)\n",
      "239. feature 83y (0.000000)\n",
      "240. feature 10z (0.000000)\n",
      "241. feature 83z (0.000000)\n",
      "242. feature 85z (0.000000)\n",
      "243. feature 34z (0.000000)\n",
      "244. feature 11x (0.000000)\n",
      "245. feature 44z (0.000000)\n",
      "246. feature 87x (0.000000)\n",
      "247. feature 53x (0.000000)\n",
      "248. feature 42x (0.000000)\n",
      "249. feature 88x (0.000000)\n",
      "250. feature 6z (0.000000)\n",
      "251. feature 88z (0.000000)\n",
      "252. feature 5x (0.000000)\n",
      "253. feature 4z (0.000000)\n",
      "254. feature 91x (0.000000)\n",
      "255. feature 93z (0.000000)\n",
      "256. feature 94z (0.000000)\n",
      "257. feature 40x (0.000000)\n",
      "258. feature 2y (0.000000)\n",
      "259. feature 95x (0.000000)\n",
      "260. feature 95z (0.000000)\n",
      "261. feature 1y (0.000000)\n",
      "262. feature 96z (0.000000)\n",
      "263. feature 11z (0.000000)\n",
      "264. feature 33z (0.000000)\n",
      "265. feature 82x (0.000000)\n",
      "266. feature 12z (0.000000)\n",
      "267. feature 65z (0.000000)\n",
      "268. feature 63x (0.000000)\n",
      "269. feature 20z (0.000000)\n",
      "270. feature 50z (0.000000)\n",
      "271. feature 21z (0.000000)\n",
      "272. feature 61z (0.000000)\n",
      "273. feature 61y (0.000000)\n",
      "274. feature 22z (0.000000)\n",
      "275. feature 29z (0.000000)\n",
      "276. feature 61x (0.000000)\n",
      "277. feature 23z (0.000000)\n",
      "278. feature 51z (0.000000)\n",
      "279. feature 59z (0.000000)\n",
      "280. feature 29x (0.000000)\n",
      "281. feature 58z (0.000000)\n",
      "282. feature 66x (0.000000)\n",
      "283. feature 47z (0.000000)\n",
      "284. feature 31z (0.000000)\n",
      "285. feature 45z (0.000000)\n",
      "286. feature 28z (0.000000)\n",
      "287. feature 80z (0.000000)\n",
      "288. feature 13z (0.000000)\n",
      "289. feature 45x (0.000000)\n",
      "290. feature 14z (0.000000)\n",
      "291. feature 76z (0.000000)\n",
      "292. feature 72z (0.000000)\n",
      "293. feature 18y (0.000000)\n",
      "294. feature 16x (0.000000)\n",
      "295. feature 71z (0.000000)\n",
      "296. feature 17x (0.000000)\n",
      "297. feature 17y (0.000000)\n",
      "298. feature 46x (0.000000)\n",
      "299. feature 46z (0.000000)\n",
      "300. feature 68z (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "col = list(X)\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, col[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model2 = RandomForestClassifier(n_estimators=50)\n",
    "cvdata = data.sample(frac=1)\n",
    "x_data = cvdata.drop(cvdata.columns[-1], axis=1)\n",
    "y_data = cvdata[cvdata.columns[-1]]\n",
    "scores = cross_val_score(model2, x_data, y_data, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 143 False Positives: 14 False Negatives: 25 True Positives: 84\n",
      "Accuracy: 0.8533834586466166 Error: 0.14661654135338342\n",
      "Precision: 0.8571428571428571 Recall: 0.7706422018348624 F1: 0.8115942028985508\n",
      "Statistics on Training\n",
      "True Negative: 456 False Positives: 35 False Negatives: 45 True Positives: 260\n",
      "Accuracy: 0.8994974874371859 Error: 0.10050251256281406\n",
      "Precision: 0.8813559322033898 Recall: 0.8524590163934426 F1: 0.8666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svcModel = SVC(kernel='linear')  \n",
    "svcModel.fit(X, y)\n",
    "print_stats(svcModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 150 False Positives: 19 False Negatives: 17 True Positives: 80\n",
      "Accuracy: 0.8646616541353384 Error: 0.13533834586466165\n",
      "Precision: 0.8080808080808081 Recall: 0.8247422680412371 F1: 0.8163265306122448\n",
      "Statistics on Training\n",
      "True Negative: 479 False Positives: 0 False Negatives: 0 True Positives: 317\n",
      "Accuracy: 1.0 Error: 0.0\n",
      "Precision: 1.0 Recall: 1.0 F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8873239436619719\n",
      "Accuracy: 0.91 (+/- 0.05)\n",
      "0.9554973821989529\n",
      "Accuracy: 0.90 (+/- 0.06)\n",
      "0.9125475285171103\n",
      "Accuracy: 0.91 (+/- 0.06)\n",
      "0.9644128113879004\n",
      "Accuracy: 0.90 (+/- 0.06)\n",
      "0.9288888888888889\n",
      "Accuracy: 0.91 (+/- 0.05)\n",
      "0.9742489270386266\n",
      "Accuracy: 0.91 (+/- 0.04)\n",
      "0.9694444444444444\n",
      "Accuracy: 0.90 (+/- 0.05)\n",
      "0.9573643410852714\n",
      "Accuracy: 0.91 (+/- 0.04)\n",
      "0.960431654676259\n",
      "Accuracy: 0.90 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "import warnings;warnings.filterwarnings('ignore')\n",
    "def train(file):\n",
    "    data = pd.read_csv(file)\n",
    "    data = data.drop(data.columns[0], axis=1)\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    X = train.drop(train.columns[-1], axis=1)\n",
    "    y = train[train.columns[-1]]\n",
    "    y_true = test[test.columns[-1]]\n",
    "    X_test = test.drop(train.columns[-1], axis=1)\n",
    "    model = RandomForestClassifier()\n",
    "    cvdata = data.sample(frac=1)\n",
    "    scores = cross_val_score(model, x_data, y_data, cv=10)\n",
    "    model.fit(X,y)\n",
    "    return (model, model.score(X_test, y_true), scores)\n",
    "files = [\"affirmative\", \"conditional\", \n",
    " \"doubt_question\", \"emphasis\", \"negative\",\"relative\",\n",
    "\"topics\", \"wh_question\", \"yn_question\"]\n",
    "models = []\n",
    "for file in files:\n",
    "    m, score, cvscores = train(\"a_\" + file + \".csv\")\n",
    "    models.append(m)\n",
    "    print(score)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cvscores.mean(), cvscores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier built on Person A tested on Person B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49162011173184356\n",
      "0.3136676499508358\n",
      "0.6820307281229125\n",
      "0.3950892857142857\n",
      "0.3419721871049305\n",
      "0.29044117647058826\n",
      "0.3293150684931507\n",
      "0.5865963855421686\n",
      "0.4108170310701956\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(files)):\n",
    "    file = files[i]\n",
    "    model = models[i]\n",
    "    forFun = pd.read_csv(\"b_\" + file + \".csv\")\n",
    "    forFun = forFun.drop(forFun.columns[0], axis=1)\n",
    "    X = forFun.drop(forFun.columns[-1], axis=1)\n",
    "    y_true = forFun[forFun.columns[-1]]\n",
    "    print(model.score(X, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"a_affirmative.csv\")\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "y = data[data.columns[-1]].iloc[1:]\n",
    "funf = data.drop(data.columns[-1], axis=1).diff().iloc[1:]\n",
    "funf = pd.concat([funf, y], axis=1)\n",
    "train, test = train_test_split(funf, test_size=0.25)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 141 False Positives: 24 False Negatives: 26 True Positives: 75\n",
      "Accuracy: 0.8120300751879699 Error: 0.18796992481203012\n",
      "Precision: 0.7575757575757576 Recall: 0.7425742574257426 F1: 0.75\n",
      "Statistics on Training\n",
      "True Negative: 482 False Positives: 0 False Negatives: 0 True Positives: 313\n",
      "Accuracy: 1.0 Error: 0.0\n",
      "Precision: 1.0 Recall: 1.0 F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X,y)\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706430568499534\n",
      "True Negative: 479 False Positives: 66 False Negatives: 249 True Positives: 279\n",
      "Precision: 0.808695652173913 Recall: 0.5284090909090909 F1: 0.6391752577319587\n",
      "Statistics on Training\n"
     ]
    }
   ],
   "source": [
    "datab = pd.read_csv(\"b_affirmative.csv\")\n",
    "datab = datab.drop(datab.columns[0], axis=1)\n",
    "y = datab[datab.columns[-1]].iloc[1:]\n",
    "X = datab.drop(datab.columns[-1], axis=1).diff().iloc[1:]\n",
    "print(model.score(X, y))\n",
    "y_pred = model.predict(X)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"True Negative:\", tn, \"False Positives:\", fp, \"False Negatives:\", fn, \"True Positives:\", tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)\n",
    "print(\"Statistics on Training\")\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets for Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.15)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "902/902 [==============================] - 1s 616us/step - loss: 0.6670 - acc: 0.6142\n",
      "Epoch 2/8\n",
      "902/902 [==============================] - 0s 135us/step - loss: 0.6654 - acc: 0.6142\n",
      "Epoch 3/8\n",
      "902/902 [==============================] - 0s 141us/step - loss: 0.6734 - acc: 0.5987\n",
      "Epoch 4/8\n",
      "902/902 [==============================] - 0s 145us/step - loss: 0.6675 - acc: 0.6131\n",
      "Epoch 5/8\n",
      "902/902 [==============================] - 0s 132us/step - loss: 0.6704 - acc: 0.6131\n",
      "Epoch 6/8\n",
      "902/902 [==============================] - 0s 153us/step - loss: 0.6703 - acc: 0.6153\n",
      "Epoch 7/8\n",
      "902/902 [==============================] - 0s 142us/step - loss: 0.6680 - acc: 0.5998\n",
      "Epoch 8/8\n",
      "902/902 [==============================] - 0s 148us/step - loss: 0.6692 - acc: 0.6109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x119653e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X.values, y.values, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Data for Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l= []\n",
    "def efg(file):\n",
    "    data = pd.read_csv(\"a_\" + file + \".csv\")\n",
    "    data = data.drop(data.columns[0], axis=1)\n",
    "    t = data[data[data.columns[-1]] == 1]\n",
    "    t[t.columns[-1]] = file\n",
    "    return t\n",
    "for file in files:\n",
    "    count = efg(file)\n",
    "    l.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.06)\n",
      "0.9730941704035875\n",
      "0.9980359147025814\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat(l)\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)\n",
    "model = RandomForestClassifier()\n",
    "scores = cross_val_score(model, x_data, y_data, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "model.fit(X,y)\n",
    "print(model.score(X_test, y_true))\n",
    "print(model.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9730941704035875\n",
      "0.9734927669050176\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "print(recall_score(y_true, y_pred, average=\"weighted\"))\n",
    "print(precision_score(y_true, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 45y (0.138088)\n",
      "2. feature 76y (0.118896)\n",
      "3. feature 68x (0.108831)\n",
      "4. feature 44z (0.090481)\n",
      "5. feature 77z (0.078817)\n",
      "6. feature 38x (0.074893)\n",
      "7. feature 41y (0.045530)\n",
      "8. feature 80z (0.045471)\n",
      "9. feature 99y (0.025123)\n",
      "10. feature 92x (0.022713)\n",
      "11. feature 82x (0.021149)\n",
      "12. feature 98x (0.020614)\n",
      "13. feature 81x (0.017445)\n",
      "14. feature 93x (0.014384)\n",
      "15. feature 76z (0.009687)\n",
      "16. feature 68y (0.009622)\n",
      "17. feature 83x (0.009506)\n",
      "18. feature 61z (0.008281)\n",
      "19. feature 18x (0.008225)\n",
      "20. feature 48z (0.006732)\n",
      "21. feature 74z (0.004608)\n",
      "22. feature 99x (0.004502)\n",
      "23. feature 70z (0.004102)\n",
      "24. feature 58y (0.003753)\n",
      "25. feature 73z (0.003689)\n",
      "26. feature 50x (0.003659)\n",
      "27. feature 72y (0.003164)\n",
      "28. feature 60z (0.003148)\n",
      "29. feature 57z (0.003137)\n",
      "30. feature 60x (0.003112)\n",
      "31. feature 39x (0.003031)\n",
      "32. feature 85y (0.003000)\n",
      "33. feature 86y (0.002991)\n",
      "34. feature 64x (0.002981)\n",
      "35. feature 97x (0.002933)\n",
      "36. feature 45z (0.002885)\n",
      "37. feature 95x (0.002770)\n",
      "38. feature 43z (0.002468)\n",
      "39. feature 52z (0.002465)\n",
      "40. feature 49z (0.002303)\n",
      "41. feature 40y (0.002286)\n",
      "42. feature 33z (0.002207)\n",
      "43. feature 47y (0.002196)\n",
      "44. feature 31z (0.002158)\n",
      "45. feature 38y (0.002124)\n",
      "46. feature 30y (0.002043)\n",
      "47. feature 64z (0.001739)\n",
      "48. feature 84x (0.001721)\n",
      "49. feature 20z (0.001661)\n",
      "50. feature 0z (0.001515)\n",
      "51. feature 53x (0.001496)\n",
      "52. feature 89y (0.001385)\n",
      "53. feature 18y (0.001349)\n",
      "54. feature 41z (0.001339)\n",
      "55. feature 70y (0.001333)\n",
      "56. feature 46z (0.001264)\n",
      "57. feature 62z (0.001257)\n",
      "58. feature 21z (0.001234)\n",
      "59. feature 59y (0.001212)\n",
      "60. feature 39z (0.001206)\n",
      "61. feature 72z (0.001205)\n",
      "62. feature 14y (0.001185)\n",
      "63. feature 12z (0.001111)\n",
      "64. feature 78z (0.001111)\n",
      "65. feature 56y (0.001088)\n",
      "66. feature 33y (0.001088)\n",
      "67. feature 21y (0.001075)\n",
      "68. feature 47x (0.001058)\n",
      "69. feature 32x (0.001007)\n",
      "70. feature 55z (0.001005)\n",
      "71. feature 94x (0.000952)\n",
      "72. feature 6y (0.000888)\n",
      "73. feature 8z (0.000832)\n",
      "74. feature 68z (0.000714)\n",
      "75. feature 9y (0.000665)\n",
      "76. feature 50z (0.000651)\n",
      "77. feature 63x (0.000635)\n",
      "78. feature 92y (0.000632)\n",
      "79. feature 28x (0.000619)\n",
      "80. feature 98z (0.000614)\n",
      "81. feature 56z (0.000605)\n",
      "82. feature 86x (0.000600)\n",
      "83. feature 92z (0.000598)\n",
      "84. feature 63y (0.000593)\n",
      "85. feature 53z (0.000590)\n",
      "86. feature 56x (0.000564)\n",
      "87. feature 11x (0.000564)\n",
      "88. feature 48x (0.000556)\n",
      "89. feature 62x (0.000544)\n",
      "90. feature 5x (0.000508)\n",
      "91. feature 31y (0.000423)\n",
      "92. feature 93z (0.000423)\n",
      "93. feature 70x (0.000423)\n",
      "94. feature 58z (0.000423)\n",
      "95. feature 94z (0.000423)\n",
      "96. feature 83z (0.000344)\n",
      "97. feature 93y (0.000317)\n",
      "98. feature 96x (0.000317)\n",
      "99. feature 81y (0.000298)\n",
      "100. feature 80x (0.000268)\n",
      "101. feature 80y (0.000209)\n",
      "102. feature 50y (0.000164)\n",
      "103. feature 99z (0.000136)\n",
      "104. feature 81z (0.000058)\n",
      "105. feature 65y (0.000000)\n",
      "106. feature 20y (0.000000)\n",
      "107. feature 23y (0.000000)\n",
      "108. feature 23x (0.000000)\n",
      "109. feature 22z (0.000000)\n",
      "110. feature 22y (0.000000)\n",
      "111. feature 22x (0.000000)\n",
      "112. feature 90y (0.000000)\n",
      "113. feature 90z (0.000000)\n",
      "114. feature 21x (0.000000)\n",
      "115. feature 91x (0.000000)\n",
      "116. feature 20x (0.000000)\n",
      "117. feature 24x (0.000000)\n",
      "118. feature 19z (0.000000)\n",
      "119. feature 19y (0.000000)\n",
      "120. feature 19x (0.000000)\n",
      "121. feature 18z (0.000000)\n",
      "122. feature 91y (0.000000)\n",
      "123. feature 91z (0.000000)\n",
      "124. feature 17z (0.000000)\n",
      "125. feature 17y (0.000000)\n",
      "126. feature 17x (0.000000)\n",
      "127. feature 23z (0.000000)\n",
      "128. feature 24y (0.000000)\n",
      "129. feature 16y (0.000000)\n",
      "130. feature 24z (0.000000)\n",
      "131. feature 88z (0.000000)\n",
      "132. feature 89x (0.000000)\n",
      "133. feature 31x (0.000000)\n",
      "134. feature 30z (0.000000)\n",
      "135. feature 89z (0.000000)\n",
      "136. feature 30x (0.000000)\n",
      "137. feature 29z (0.000000)\n",
      "138. feature 29y (0.000000)\n",
      "139. feature 29x (0.000000)\n",
      "140. feature 28z (0.000000)\n",
      "141. feature 28y (0.000000)\n",
      "142. feature 90x (0.000000)\n",
      "143. feature 27z (0.000000)\n",
      "144. feature 27y (0.000000)\n",
      "145. feature 27x (0.000000)\n",
      "146. feature 26z (0.000000)\n",
      "147. feature 26y (0.000000)\n",
      "148. feature 26x (0.000000)\n",
      "149. feature 25z (0.000000)\n",
      "150. feature 25y (0.000000)\n",
      "151. feature 25x (0.000000)\n",
      "152. feature 16z (0.000000)\n",
      "153. feature 16x (0.000000)\n",
      "154. feature 32y (0.000000)\n",
      "155. feature 7z (0.000000)\n",
      "156. feature 7x (0.000000)\n",
      "157. feature 6z (0.000000)\n",
      "158. feature 97y (0.000000)\n",
      "159. feature 6x (0.000000)\n",
      "160. feature 5z (0.000000)\n",
      "161. feature 5y (0.000000)\n",
      "162. feature 97z (0.000000)\n",
      "163. feature 4z (0.000000)\n",
      "164. feature 4y (0.000000)\n",
      "165. feature 4x (0.000000)\n",
      "166. feature 3z (0.000000)\n",
      "167. feature 3y (0.000000)\n",
      "168. feature 3x (0.000000)\n",
      "169. feature 2z (0.000000)\n",
      "170. feature 2y (0.000000)\n",
      "171. feature 2x (0.000000)\n",
      "172. feature 1z (0.000000)\n",
      "173. feature 1y (0.000000)\n",
      "174. feature 1x (0.000000)\n",
      "175. feature 98y (0.000000)\n",
      "176. feature 0y (0.000000)\n",
      "177. feature 7y (0.000000)\n",
      "178. feature 8x (0.000000)\n",
      "179. feature 15z (0.000000)\n",
      "180. feature 8y (0.000000)\n",
      "181. feature 15y (0.000000)\n",
      "182. feature 15x (0.000000)\n",
      "183. feature 14z (0.000000)\n",
      "184. feature 94y (0.000000)\n",
      "185. feature 14x (0.000000)\n",
      "186. feature 13z (0.000000)\n",
      "187. feature 13y (0.000000)\n",
      "188. feature 13x (0.000000)\n",
      "189. feature 95y (0.000000)\n",
      "190. feature 12y (0.000000)\n",
      "191. feature 12x (0.000000)\n",
      "192. feature 11z (0.000000)\n",
      "193. feature 11y (0.000000)\n",
      "194. feature 95z (0.000000)\n",
      "195. feature 10z (0.000000)\n",
      "196. feature 10y (0.000000)\n",
      "197. feature 10x (0.000000)\n",
      "198. feature 9z (0.000000)\n",
      "199. feature 96y (0.000000)\n",
      "200. feature 9x (0.000000)\n",
      "201. feature 96z (0.000000)\n",
      "202. feature 88y (0.000000)\n",
      "203. feature 32z (0.000000)\n",
      "204. feature 65x (0.000000)\n",
      "205. feature 49y (0.000000)\n",
      "206. feature 72x (0.000000)\n",
      "207. feature 73x (0.000000)\n",
      "208. feature 73y (0.000000)\n",
      "209. feature 74x (0.000000)\n",
      "210. feature 55y (0.000000)\n",
      "211. feature 55x (0.000000)\n",
      "212. feature 54z (0.000000)\n",
      "213. feature 54y (0.000000)\n",
      "214. feature 54x (0.000000)\n",
      "215. feature 74y (0.000000)\n",
      "216. feature 53y (0.000000)\n",
      "217. feature 75x (0.000000)\n",
      "218. feature 75y (0.000000)\n",
      "219. feature 52y (0.000000)\n",
      "220. feature 52x (0.000000)\n",
      "221. feature 51z (0.000000)\n",
      "222. feature 51y (0.000000)\n",
      "223. feature 51x (0.000000)\n",
      "224. feature 75z (0.000000)\n",
      "225. feature 76x (0.000000)\n",
      "226. feature 77x (0.000000)\n",
      "227. feature 57x (0.000000)\n",
      "228. feature 57y (0.000000)\n",
      "229. feature 71z (0.000000)\n",
      "230. feature 67z (0.000000)\n",
      "231. feature 65z (0.000000)\n",
      "232. feature 64y (0.000000)\n",
      "233. feature 66x (0.000000)\n",
      "234. feature 63z (0.000000)\n",
      "235. feature 66y (0.000000)\n",
      "236. feature 66z (0.000000)\n",
      "237. feature 67x (0.000000)\n",
      "238. feature 62y (0.000000)\n",
      "239. feature 67y (0.000000)\n",
      "240. feature 61y (0.000000)\n",
      "241. feature 58x (0.000000)\n",
      "242. feature 61x (0.000000)\n",
      "243. feature 69x (0.000000)\n",
      "244. feature 60y (0.000000)\n",
      "245. feature 69y (0.000000)\n",
      "246. feature 59z (0.000000)\n",
      "247. feature 69z (0.000000)\n",
      "248. feature 59x (0.000000)\n",
      "249. feature 71x (0.000000)\n",
      "250. feature 71y (0.000000)\n",
      "251. feature 77y (0.000000)\n",
      "252. feature 49x (0.000000)\n",
      "253. feature 33x (0.000000)\n",
      "254. feature 78x (0.000000)\n",
      "255. feature 40x (0.000000)\n",
      "256. feature 85z (0.000000)\n",
      "257. feature 39y (0.000000)\n",
      "258. feature 86z (0.000000)\n",
      "259. feature 38z (0.000000)\n",
      "260. feature 87x (0.000000)\n",
      "261. feature 87y (0.000000)\n",
      "262. feature 37z (0.000000)\n",
      "263. feature 37y (0.000000)\n",
      "264. feature 37x (0.000000)\n",
      "265. feature 36z (0.000000)\n",
      "266. feature 36y (0.000000)\n",
      "267. feature 36x (0.000000)\n",
      "268. feature 35z (0.000000)\n",
      "269. feature 35y (0.000000)\n",
      "270. feature 35x (0.000000)\n",
      "271. feature 34z (0.000000)\n",
      "272. feature 34y (0.000000)\n",
      "273. feature 34x (0.000000)\n",
      "274. feature 87z (0.000000)\n",
      "275. feature 88x (0.000000)\n",
      "276. feature 85x (0.000000)\n",
      "277. feature 40z (0.000000)\n",
      "278. feature 41x (0.000000)\n",
      "279. feature 45x (0.000000)\n",
      "280. feature 48y (0.000000)\n",
      "281. feature 78y (0.000000)\n",
      "282. feature 47z (0.000000)\n",
      "283. feature 79x (0.000000)\n",
      "284. feature 79y (0.000000)\n",
      "285. feature 79z (0.000000)\n",
      "286. feature 46y (0.000000)\n",
      "287. feature 46x (0.000000)\n",
      "288. feature 82y (0.000000)\n",
      "289. feature 82z (0.000000)\n",
      "290. feature 84z (0.000000)\n",
      "291. feature 44y (0.000000)\n",
      "292. feature 44x (0.000000)\n",
      "293. feature 83y (0.000000)\n",
      "294. feature 43y (0.000000)\n",
      "295. feature 43x (0.000000)\n",
      "296. feature 42z (0.000000)\n",
      "297. feature 42y (0.000000)\n",
      "298. feature 42x (0.000000)\n",
      "299. feature 84y (0.000000)\n",
      "300. feature 0x (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "col = list(X)\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, col[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.06)\n",
      "0.9349775784753364\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "scores = cross_val_score(model2, x_data, y_data, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "model.fit(X,y)\n",
    "print(model.score(X_test, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9349775784753364\n",
      "0.9381452528537729\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "print(recall_score(y_true, y_pred, average=\"weighted\"))\n",
    "print(precision_score(y_true, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(y):\n",
    "    files = [\"affirmative\", \"conditional\", \n",
    " \"doubt_question\", \"emphasis\", \"negative\",\"relative\",\n",
    "\"topics\", \"wh_question\", \"yn_question\"]\n",
    "    mapped = {}\n",
    "    for i in range(len(files)):\n",
    "        mapped[files[i]] = i\n",
    "    value = [0 for i in range(len(files))]\n",
    "    ans = []\n",
    "    for i in y:\n",
    "        hot = value[:]\n",
    "        hot[mapped[i]] = 1\n",
    "        ans.append(hot)\n",
    "    return np.asarray(ans)\n",
    "dummy_y = convert(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3564/3564 [==============================] - 1s 235us/step - loss: 2.1916 - acc: 0.1330\n",
      "Epoch 2/5\n",
      "3564/3564 [==============================] - 0s 123us/step - loss: 2.1887 - acc: 0.1299\n",
      "Epoch 3/5\n",
      "3564/3564 [==============================] - 1s 160us/step - loss: 2.1842 - acc: 0.1375\n",
      "Epoch 4/5\n",
      "3564/3564 [==============================] - 1s 179us/step - loss: 2.1816 - acc: 0.1386\n",
      "Epoch 5/5\n",
      "3564/3564 [==============================] - 1s 159us/step - loss: 2.1834 - acc: 0.1254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1193ed198>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(200, activation=\"tanh\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(9, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X.values, np.asarray(dummy_y), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model against other person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efg(person, file):\n",
    "    data = pd.read_csv(person + \"_\" + file + \".csv\")\n",
    "    data = data.drop(data.columns[0], axis=1)\n",
    "    y = data[data.columns[-1]].iloc[1:]\n",
    "    funf = data.drop(data.columns[-1], axis=1).diff().iloc[1:]\n",
    "    funf = pd.concat([funf, y], axis=1)\n",
    "    t = funf[funf[funf.columns[-1]] == 1]\n",
    "    t[t.columns[-1]] = file\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l= []\n",
    "for file in files:\n",
    "    count = efg(\"a\", file)\n",
    "    l.append(count)\n",
    "data_a = pd.concat(l)\n",
    "l= []\n",
    "for file in files:\n",
    "    count = efg(\"b\", file)\n",
    "    l.append(count)\n",
    "data_b = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_a, test_size=0.25)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.533213644524237\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X,y)\n",
    "print(model.score(X_test, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16955719557195573"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_b = data_b[data_b.columns[-1]].iloc[1:]\n",
    "X_b = data_b.drop(data_b.columns[-1], axis=1).diff().iloc[1:]\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
