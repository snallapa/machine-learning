{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"spambase/spambase.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.25)\n",
    "X = train.drop(train.columns[-1], axis=1)\n",
    "y = train[train.columns[-1]]\n",
    "y_true = test[test.columns[-1]]\n",
    "X_test = test.drop(train.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(model):\n",
    "    print(\"Statistics on Testing\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"True Negative:\", tn, \"False Positives:\", fp, \"False Negatives:\", fn, \"True Positives:\", tp)\n",
    "    accuracy = model.score(X_test, y_true)\n",
    "    error = 1 - accuracy\n",
    "    print(\"Accuracy:\", accuracy, \"Error:\", error)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)\n",
    "    print(\"Statistics on Training\")\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"True Negative:\", tn, \"False Positives:\", fp, \"False Negatives:\", fn, \"True Positives:\", tp)\n",
    "    accuracy = model.score(X, y)\n",
    "    error = 1 - accuracy\n",
    "    print(\"Accuracy:\", accuracy, \"Error:\", error)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 667 False Positives: 18 False Negatives: 49 True Positives: 416\n",
      "Accuracy: 0.9417391304347826 Error: 0.058260869565217366\n",
      "Precision: 0.9585253456221198 Recall: 0.8946236559139785 F1: 0.9254727474972192\n",
      "Statistics on Training\n",
      "True Negative: 2099 False Positives: 4 False Negatives: 5 True Positives: 1342\n",
      "Accuracy: 0.9973913043478261 Error: 0.00260869565217392\n",
      "Precision: 0.9970282317979198 Recall: 0.9962880475129918 F1: 0.9966580022279985\n"
     ]
    }
   ],
   "source": [
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import log\n",
    "from math import isnan\n",
    "class Node:\n",
    "    def __init__(self, col, val, isLeaf):\n",
    "        self.col = col\n",
    "        self.val = val\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.isLeaf = isLeaf\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.isLeaf:\n",
    "            return self.val\n",
    "        lOrR = x[str(self.col)] >= self.val\n",
    "        if lOrR:\n",
    "            return self.right.predict(x)\n",
    "        else:\n",
    "            return self.left.predict(x)\n",
    "def entropy(ps):\n",
    "    s = 0\n",
    "    for p in ps:\n",
    "        if p <= 0.000001:\n",
    "            continue\n",
    "        s += p * log(p,2)\n",
    "    return -1 * s\n",
    "def information_gain(data, col, val):\n",
    "    part1 = data.loc[data[col] >= val]\n",
    "    if len(part1) == 0:\n",
    "        p = 0\n",
    "    else:\n",
    "        c = np.sum(part1.iloc[:,-1])\n",
    "        p = c/len(part1)\n",
    "    rightEntropy = entropy([p, 1-p])\n",
    "    part2 = data.loc[data[col] < val]\n",
    "    if len(part2) == 0:\n",
    "        p == 0\n",
    "    else:\n",
    "        c = np.sum(part2.iloc[:,-1])\n",
    "        p = c/len(part2)\n",
    "    leftEntropy = entropy([p, 1-p])\n",
    "    cond = len(part1)/len(data) * rightEntropy + len(part2)/len(data) * leftEntropy\n",
    "    cWhole = np.sum(data.iloc[:,-1])\n",
    "    pWhole = cWhole/len(data)\n",
    "    return entropy([pWhole, 1-pWhole]) - cond\n",
    "class RF:\n",
    "    def __init__(self, trees, m, samples=1000):\n",
    "        self.trees = trees\n",
    "        self.m = m\n",
    "        self.samples = 1000\n",
    "    def train(self, train):\n",
    "        trees = self.trees\n",
    "        m = self.m\n",
    "        t = []\n",
    "        def build_tree(data, layers):\n",
    "            xs = data.drop(train.columns[-1], axis=1)\n",
    "            ys = data[data.columns[-1]]\n",
    "            test = np.sum(ys)\n",
    "            if test == len(ys) or test == 0:\n",
    "                return Node(None, 0 if test == 0 else 1, True)\n",
    "            if layers == 1:\n",
    "                return Node(None, 1 if test > len(ys)/2 else 0, True)\n",
    "            selected = xs.sample(m, axis=1).join(ys)\n",
    "            bestIg = -1\n",
    "            bestCol = None\n",
    "            for col in selected.columns[:-1]:\n",
    "                sep = selected[col].unique()\n",
    "                if len(sep) > 5:\n",
    "                    _, sep = np.histogram(sep, bins=5)\n",
    "                for val in sep:\n",
    "                    ig = information_gain(selected, col, val)\n",
    "                    if ig > bestIg:\n",
    "                        bestIg = ig\n",
    "                        bestCol = Node(col, val, False)\n",
    "            left = data[data[bestCol.col] < bestCol.val]\n",
    "            right = data[data[bestCol.col] >= bestCol.val]\n",
    "            bestCol.left = build_tree(left, layers - 1)\n",
    "            bestCol.right = build_tree(right, layers - 1)\n",
    "            return bestCol\n",
    "        for i in range(trees):\n",
    "            s = resample(train, n_samples=self.samples)\n",
    "            t.append(build_tree(s, 10))\n",
    "        self.t = t\n",
    "    def predict(self, x_test):\n",
    "        def pr(x):\n",
    "            s = 0\n",
    "            for i in self.t:\n",
    "                s += i.predict(x)\n",
    "            return 1 if s >= (len(self.t)/2) else 0\n",
    "        return x_test.apply(pr, axis=1)\n",
    "    def score(self, x, y_true):\n",
    "        y_pred = self.predict(x)\n",
    "        return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 is done\n",
      "Tree 2 is done\n",
      "Tree 3 is done\n",
      "Tree 4 is done\n",
      "Tree 5 is done\n"
     ]
    }
   ],
   "source": [
    "m = len(X.columns)\n",
    "myModel = RF(5, m)\n",
    "myModel.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 670 False Positives: 15 False Negatives: 94 True Positives: 371\n",
      "Accuracy: 0.9052173913043479 Error: 0.09478260869565214\n",
      "Precision: 0.961139896373057 Recall: 0.7978494623655914 F1: 0.8719153936545242\n",
      "Statistics on Training\n",
      "True Negative: 2046 False Positives: 57 False Negatives: 210 True Positives: 1137\n",
      "Accuracy: 0.922608695652174 Error: 0.07739130434782604\n",
      "Precision: 0.9522613065326633 Recall: 0.844097995545657 F1: 0.8949232585596221\n"
     ]
    }
   ],
   "source": [
    "print_stats(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 is done\n",
      "Tree 2 is done\n",
      "Tree 3 is done\n",
      "Tree 4 is done\n",
      "Tree 5 is done\n"
     ]
    }
   ],
   "source": [
    "d = len(X.columns)\n",
    "myModel = RF(5, d//2, 500)\n",
    "myModel.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 659 False Positives: 26 False Negatives: 89 True Positives: 376\n",
      "Accuracy: 0.9 Error: 0.09999999999999998\n",
      "Precision: 0.9353233830845771 Recall: 0.8086021505376344 F1: 0.8673587081891581\n",
      "Statistics on Training\n",
      "True Negative: 2046 False Positives: 57 False Negatives: 214 True Positives: 1133\n",
      "Accuracy: 0.9214492753623188 Error: 0.0785507246376812\n",
      "Precision: 0.9521008403361344 Recall: 0.8411284335560505 F1: 0.8931809223492313\n"
     ]
    }
   ],
   "source": [
    "print_stats(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 is done\n",
      "Tree 2 is done\n",
      "Tree 3 is done\n",
      "Tree 4 is done\n",
      "Tree 5 is done\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "d = len(X.columns)\n",
    "myModel = RF(5, int(sqrt(d)))\n",
    "myModel.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 661 False Positives: 24 False Negatives: 98 True Positives: 367\n",
      "Accuracy: 0.8939130434782608 Error: 0.10608695652173916\n",
      "Precision: 0.9386189258312021 Recall: 0.789247311827957 F1: 0.8574766355140188\n",
      "Statistics on Training\n",
      "True Negative: 2034 False Positives: 69 False Negatives: 237 True Positives: 1110\n",
      "Accuracy: 0.9113043478260869 Error: 0.08869565217391306\n",
      "Precision: 0.9414758269720102 Recall: 0.8240534521158129 F1: 0.8788598574821853\n"
     ]
    }
   ],
   "source": [
    "print_stats(myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 is done\n",
      "Tree 2 is done\n",
      "Tree 3 is done\n",
      "Tree 4 is done\n",
      "Tree 5 is done\n",
      "Tree 6 is done\n",
      "Tree 7 is done\n",
      "Tree 8 is done\n",
      "Tree 9 is done\n",
      "Tree 10 is done\n",
      "\n",
      "Stats of Mine\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 666 False Positives: 19 False Negatives: 82 True Positives: 383\n",
      "Accuracy: 0.9121739130434783 Error: 0.08782608695652172\n",
      "Precision: 0.9527363184079602 Recall: 0.8236559139784946 F1: 0.8835063437139561\n",
      "Statistics on Training\n",
      "True Negative: 2043 False Positives: 60 False Negatives: 187 True Positives: 1160\n",
      "Accuracy: 0.9284057971014493 Error: 0.07159420289855067\n",
      "Precision: 0.9508196721311475 Recall: 0.8611729769858946 F1: 0.9037787300350604\n",
      "Stats of Package\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 667 False Positives: 18 False Negatives: 46 True Positives: 419\n",
      "Accuracy: 0.9443478260869566 Error: 0.055652173913043446\n",
      "Precision: 0.9588100686498856 Recall: 0.9010752688172043 F1: 0.9290465631929047\n",
      "Statistics on Training\n",
      "True Negative: 2097 False Positives: 6 False Negatives: 8 True Positives: 1339\n",
      "Accuracy: 0.9959420289855072 Error: 0.004057971014492789\n",
      "Precision: 0.995539033457249 Recall: 0.994060876020787 F1: 0.9947994056463595\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "d = len(X.columns)\n",
    "m = int(sqrt(d))\n",
    "myModel = RF(10, m)\n",
    "myModel.train(train)\n",
    "y_pred = myModel.predict(X_test)\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X,y)\n",
    "print(\"\\nStats of Mine\\n\")\n",
    "print_stats(myModel)\n",
    "print(\"Stats of Package\\n\")\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 is done\n",
      "Tree 2 is done\n",
      "Tree 3 is done\n",
      "Tree 4 is done\n",
      "Tree 5 is done\n",
      "Tree 6 is done\n",
      "Tree 7 is done\n",
      "Tree 8 is done\n",
      "Tree 9 is done\n",
      "Tree 10 is done\n",
      "Tree 11 is done\n",
      "Tree 12 is done\n",
      "Tree 13 is done\n",
      "Tree 14 is done\n",
      "Tree 15 is done\n",
      "Tree 16 is done\n",
      "Tree 17 is done\n",
      "Tree 18 is done\n",
      "Tree 19 is done\n",
      "Tree 20 is done\n",
      "Tree 21 is done\n",
      "Tree 22 is done\n",
      "Tree 23 is done\n",
      "Tree 24 is done\n",
      "Tree 25 is done\n",
      "Tree 26 is done\n",
      "Tree 27 is done\n",
      "Tree 28 is done\n",
      "Tree 29 is done\n",
      "Tree 30 is done\n",
      "Tree 31 is done\n",
      "Tree 32 is done\n",
      "Tree 33 is done\n",
      "Tree 34 is done\n",
      "Tree 35 is done\n",
      "Tree 36 is done\n",
      "Tree 37 is done\n",
      "Tree 38 is done\n",
      "Tree 39 is done\n",
      "Tree 40 is done\n",
      "Tree 41 is done\n",
      "Tree 42 is done\n",
      "Tree 43 is done\n",
      "Tree 44 is done\n",
      "Tree 45 is done\n",
      "Tree 46 is done\n",
      "Tree 47 is done\n",
      "Tree 48 is done\n",
      "Tree 49 is done\n",
      "Tree 50 is done\n",
      "\n",
      "Stats of Mine\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 677 False Positives: 8 False Negatives: 94 True Positives: 371\n",
      "Accuracy: 0.9113043478260869 Error: 0.08869565217391306\n",
      "Precision: 0.978891820580475 Recall: 0.7978494623655914 F1: 0.8791469194312795\n",
      "Statistics on Training\n",
      "True Negative: 2068 False Positives: 35 False Negatives: 218 True Positives: 1129\n",
      "Accuracy: 0.9266666666666666 Error: 0.07333333333333336\n",
      "Precision: 0.9699312714776632 Recall: 0.838158871566444 F1: 0.8992433293508563\n",
      "Stats of Package\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 669 False Positives: 16 False Negatives: 38 True Positives: 427\n",
      "Accuracy: 0.9530434782608695 Error: 0.04695652173913045\n",
      "Precision: 0.963882618510158 Recall: 0.9182795698924732 F1: 0.9405286343612336\n",
      "Statistics on Training\n",
      "True Negative: 2101 False Positives: 2 False Negatives: 3 True Positives: 1344\n",
      "Accuracy: 0.9985507246376811 Error: 0.0014492753623188692\n",
      "Precision: 0.9985141158989599 Recall: 0.9977728285077951 F1: 0.9981433345711103\n"
     ]
    }
   ],
   "source": [
    "d = len(X.columns)\n",
    "m = int(sqrt(d))\n",
    "myModel = RF(50, m, 200)\n",
    "myModel.train(train)\n",
    "y_pred = myModel.predict(X_test)\n",
    "model = RandomForestClassifier(n_estimators=50)\n",
    "model.fit(X,y)\n",
    "print(\"\\nStats of Mine\\n\")\n",
    "print_stats(myModel)\n",
    "print(\"Stats of Package\\n\")\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of Mine\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 675 False Positives: 10 False Negatives: 140 True Positives: 325\n",
      "Accuracy: 0.8695652173913043 Error: 0.13043478260869568\n",
      "Precision: 0.9701492537313433 Recall: 0.6989247311827957 F1: 0.8125\n",
      "Statistics on Training\n",
      "True Negative: 2077 False Positives: 26 False Negatives: 372 True Positives: 975\n",
      "Accuracy: 0.8846376811594203 Error: 0.11536231884057968\n",
      "Precision: 0.974025974025974 Recall: 0.7238307349665924 F1: 0.8304940374787053\n",
      "Stats of Package\n",
      "\n",
      "Statistics on Testing\n",
      "True Negative: 672 False Positives: 13 False Negatives: 40 True Positives: 425\n",
      "Accuracy: 0.9539130434782609 Error: 0.04608695652173911\n",
      "Precision: 0.9703196347031964 Recall: 0.9139784946236559 F1: 0.9413067552602437\n",
      "Statistics on Training\n",
      "True Negative: 2102 False Positives: 1 False Negatives: 2 True Positives: 1345\n",
      "Accuracy: 0.9991304347826087 Error: 0.0008695652173913437\n",
      "Precision: 0.9992570579494799 Recall: 0.9985152190051967 F1: 0.9988860007426661\n"
     ]
    }
   ],
   "source": [
    "d = len(X.columns)\n",
    "m = int(sqrt(d))\n",
    "myModel = RF(100, m, 100)\n",
    "myModel.train(train)\n",
    "y_pred = myModel.predict(X_test)\n",
    "model = RandomForestClassifier(n_estimators=50)\n",
    "model.fit(X,y)\n",
    "print(\"\\nStats of Mine\\n\")\n",
    "print_stats(myModel)\n",
    "print(\"Stats of Package\\n\")\n",
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation is definitely slower than the package's as I had to use smaller bootstrapping size to get results. However, the accuracy is close to that of the package as I can get around 90% while the package delivers 94%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 659 False Positives: 26 False Negatives: 44 True Positives: 421\n",
      "Accuracy: 0.9391304347826087 Error: 0.060869565217391286\n",
      "Precision: 0.941834451901566 Recall: 0.9053763440860215 F1: 0.9232456140350879\n",
      "Statistics on Training\n",
      "True Negative: 2020 False Positives: 83 False Negatives: 91 True Positives: 1256\n",
      "Accuracy: 0.9495652173913044 Error: 0.050434782608695605\n",
      "Precision: 0.9380134428678119 Recall: 0.9324424647364514 F1: 0.9352196574832464\n"
     ]
    }
   ],
   "source": [
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=0))\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on Testing\n",
      "True Negative: 651 False Positives: 34 False Negatives: 73 True Positives: 392\n",
      "Accuracy: 0.9069565217391304 Error: 0.09304347826086956\n",
      "Precision: 0.92018779342723 Recall: 0.843010752688172 F1: 0.8799102132435466\n",
      "Statistics on Training\n",
      "True Negative: 1991 False Positives: 112 False Negatives: 169 True Positives: 1178\n",
      "Accuracy: 0.9185507246376812 Error: 0.08144927536231883\n",
      "Precision: 0.9131782945736434 Recall: 0.874536005939124 F1: 0.8934395145999242\n"
     ]
    }
   ],
   "source": [
    "print_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers: 10 Accuracy: 0.9095652173913044 Error: 0.07826086956521738\n",
      "Classifiers: 50 Accuracy: 0.9217391304347826 Error: 0.07826086956521738\n",
      "Classifiers: 100 Accuracy: 0.9217391304347826 Error: 0.07826086956521738\n"
     ]
    }
   ],
   "source": [
    "numClassifiers = [10,50,100]\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "for i in numClassifiers:\n",
    "    model = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=0), n_estimators=i)\n",
    "    model.fit(X,y)\n",
    "    accuracy = model.score(X_test, y_true)\n",
    "    print(\"Classifiers:\",i, \"Accuracy:\", accuracy, \"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy did increase as more estimators were added but it seemed to be to a certain limit. Even adding 50 more classfiers did not increase the accuracy after the 50 mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost model gave about 92% accuracy with 100 estimators while the RandomForestModel gave 95% accuracy with 100 random trees. The Random Forest was definitely better as both those scores were on the testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A and Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.6492 - acc: 0.8381\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3363 - acc: 0.9060\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.2863 - acc: 0.9197\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.2547 - acc: 0.9283\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2317 - acc: 0.9351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1251e67f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fast forward neural network had basically two layers. One layer had 128 nodes with an activation function of relu while the next had 10 nodes with a soft max activation function. The accuracy was high with 93%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.3023 - acc: 0.1527\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 2.2801 - acc: 0.1297\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.2514 - acc: 0.1565\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 2.1920 - acc: 0.2322\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.0828 - acc: 0.4113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123b30c18>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\")\n",
    "])\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n",
      "Accuracy: 0.4384\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fast forward neural network had basically three layers, each layer was the same with 10 nodes and the activation function of sigmoid. Accuracy was low with only 40% on testing data and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 2.2861 - acc: 0.1596\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 2.1507 - acc: 0.4887\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 1.6126 - acc: 0.6832\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 1.0148 - acc: 0.7764\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7316 - acc: 0.8271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bf125f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(128, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\")\n",
    "])\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Accuracy: 0.8463\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fast forward neural network had basically three layers. The first layer and second layer had 128 nodes with activation function of sigmoid. The last layer only had 10 nodes with activation function of sigmoid as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C and Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.4309 - acc: 0.8754\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 193s 3ms/step - loss: 0.2487 - acc: 0.9302\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 169s 3ms/step - loss: 0.1421 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126aa1978>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s 857us/step\n",
      "Accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3 layered network with 2 Convolutional layers, one with 64 nodes and the other with 32. The filter size was 3 on each (3x3) with activation function of relu. The last layer was a dense layer with 10 nodes and a softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 29s 488us/step - loss: 1.5247 - acc: 0.5225\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 28s 473us/step - loss: 0.3724 - acc: 0.8937\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 0.2893 - acc: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129ce1d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=6, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.Conv2D(4, kernel_size=2, strides=(2,1), activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 270us/step\n",
      "Accuracy: 0.9295\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3 layered network with 2 Convolutional layers, one with 16 nodes and the other with 4. The filter size was 6 on on (6x6) and the other with 2 (2x2) with stride of 2 with activation function of relu. The last layer was a dense layer with 10 nodes and a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 28s 463us/step - loss: 6.8311 - acc: 0.5685\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 26s 437us/step - loss: 2.0435 - acc: 0.8645\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 1.8580 - acc: 0.8790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1229d4438>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=6, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=None))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 283us/step\n",
      "Accuracy: 0.8695\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3 layered network with 1 Convolutional layers and 1 max pooling layer. The first layer is a 16 node convolutional layer with filter size 6x6. The second layer is a max pooling with size 3x3. The last layer is 10 node layer for the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convulational layer performed better as the model had accuracy of 97% with two layers of convlutional and one for output while fast forward had 94%. These were tested on the test data. However, the Convulational took much longer to train. For each epoch, it took about 30s to completely train over the training set. While the fast forward took only about 5s per epoch. Especially on my machine, this made it possible to train more epochs with the fast forward neural net than it did for the convulational."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
